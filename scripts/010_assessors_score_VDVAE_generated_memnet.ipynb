{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T11:43:55.445759Z",
     "start_time": "2025-04-24T11:43:55.441409Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sys\n",
    "sys.path.append('C:/Users/sOrOush/SoroushProjects/01_Soroush_and_Shakiba/NSD_High_Dimensional_Data/11_Marco_And_Soroush/Scripts/GANalyze/pytorch')"
   ],
   "id": "dcafb84c47137f3",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T11:44:04.510477Z",
     "start_time": "2025-04-24T11:44:04.418988Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "mean = np.load('C:/Users/sOrOush/SoroushProjects/01_Soroush_and_Shakiba/NSD_High_Dimensional_Data/11_Marco_And_Soroush/Data/GANalyze/pytorch/assessors/image_mean.npy')"
   ],
   "id": "db9cd4384c58b8f2",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T11:57:24.946589Z",
     "start_time": "2025-04-24T11:57:24.940963Z"
    }
   },
   "cell_type": "code",
   "source": "from assessors import memnet\n",
   "id": "4d7a876594c14d0f",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T12:12:02.562116Z",
     "start_time": "2025-04-24T12:12:02.016576Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import socket\n",
    "import argparse\n",
    "import json\n",
    "import subprocess\n",
    "from contextlib import contextmanager\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel.distributed import DistributedDataParallel\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "import pickle\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sub = 1\n",
    "bs = 30"
   ],
   "id": "88fcf93c1c2c20f5",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T12:13:26.578733Z",
     "start_time": "2025-04-24T12:13:22.186826Z"
    }
   },
   "cell_type": "code",
   "source": "assessor = memnet.MemNet()",
   "id": "338b13b136f1f3f7",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users/sOrOush/SoroushProjects/01_Soroush_and_Shakiba/NSD_High_Dimensional_Data/11_Marco_And_Soroush/Scripts/GANalyze/pytorch\\assessors\\memnet.py:62: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  state_dict = pickle.load(f)\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T12:13:28.850075Z",
     "start_time": "2025-04-24T12:13:28.846280Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def test_images_prep_assessor_CLIP(images, target_size=(256, 256), mean=None):\n",
    "    data_tensor = []\n",
    "\n",
    "    # Define the transformation: resizing, channel reordering, mean subtraction, cropping, and converting to tensor\n",
    "    transform = T.Compose([\n",
    "        T.Resize(target_size),\n",
    "        T.Lambda(lambda x: np.array(x)),  # Convert image to numpy array\n",
    "        T.Lambda(lambda x: np.subtract(x[:,:,[2, 1, 0]], mean) if mean is not None else x),  # Subtract average mean from image (BGR order if mean is provided)\n",
    "        T.Lambda(lambda x: x[15:242, 15:242]),  # Center crop\n",
    "        T.ToTensor()  # Convert to tensor and scale to [0, 1]\n",
    "    ])\n",
    "\n",
    "    # Iterate over each in-memory image\n",
    "    for im in images:\n",
    "        # Apply the transformations (resize, mean subtraction, cropping, and to tensor)\n",
    "        img_tensor = transform(im)\n",
    "        data_tensor.append(img_tensor)\n",
    "\n",
    "    # Stack the list of tensors into a single tensor\n",
    "    return torch.stack(data_tensor)"
   ],
   "id": "c898f52a35922dcc",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T12:13:51.644900Z",
     "start_time": "2025-04-24T12:13:51.639475Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Transfor latents from flattened representation to hierarchical\n",
    "def latent_transformation(latents, ref):\n",
    "  layer_dims = np.array([2**4,2**4,2**8,2**8,2**8,2**8,2**10,2**10,2**10,2**10,2**10,2**10,2**10,2**10,2**12,2**12,2**12,2**12,2**12,2**12,2**12,2**12,2**12,2**12,2**12,2**12,2**12,2**12,2**12,2**12,2**14])\n",
    "  transformed_latents = []\n",
    "  for i in range(31):\n",
    "    t_lat = latents[:,layer_dims[:i].sum():layer_dims[:i+1].sum()]\n",
    "    #std_norm_test_latent = (t_lat - np.mean(t_lat,axis=0)) / np.std(t_lat,axis=0)\n",
    "    #renorm_test_latent = std_norm_test_latent * np.std(kamitani_latents[i][num_test:].reshape(num_train,-1),axis=0) + np.mean(kamitani_latents[i][num_test:].reshape(num_train,-1),axis=0)\n",
    "    c,h,w=ref[i]['z'].shape[1:]\n",
    "    transformed_latents.append(t_lat.reshape(len(latents),c,h,w))\n",
    "  return transformed_latents\n",
    "\n",
    "# idx = range(len(test_images))\n",
    "# input_latent = latent_transformation(pred_latents[idx],ref_latent)\n",
    "\n",
    "\n",
    "def sample_from_hier_latents(latents,sample_ids):\n",
    "  sample_ids = [id for id in sample_ids if id<len(latents[0])]\n",
    "  layers_num=len(latents)\n",
    "  sample_latents = []\n",
    "  for i in range(layers_num):\n",
    "    sample_latents.append(torch.tensor(latents[i][sample_ids]).float().cuda())\n",
    "  return sample_latents\n"
   ],
   "id": "6984306bdfe75c46",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T12:16:03.449085Z",
     "start_time": "2025-04-24T12:16:03.444027Z"
    }
   },
   "cell_type": "code",
   "source": [
    "emonet_res = {\n",
    "    'nsdgeneral_degree1': [],\n",
    "    'nsdgeneral_degree2': [],\n",
    "    'nsdgeneral_degree3': [],\n",
    "    'nsdgeneral_degree4': [],\n",
    "    'Default_degree1': [],\n",
    "    'Default_degree2': [],\n",
    "    'Default_degree3': [],\n",
    "    'Default_degree4': [],\n",
    "    'Auditory_degree1': [],\n",
    "    'Auditory_degree2': [],\n",
    "    'Auditory_degree3': [],\n",
    "    'Auditory_degree4': []\n",
    "}"
   ],
   "id": "263f128afaa0300b",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T12:16:16.765203Z",
     "start_time": "2025-04-24T12:16:16.114475Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## real_img\n",
    "image_path = r\"C:\\Users\\sOrOush\\SoroushProjects\\14_CLIP_Ozcelic\\results\\00_Original_Images\"\n",
    "test_tensor = test_images_prep_assessor_CLIP(image_path)\n",
    "# Reshape while keeping the last 3 dimensions\n",
    "test_im = test_tensor.view(-1, *test_tensor.shape[-3:])\n",
    "\n",
    "#------  Apply the transformations and assess\n",
    "score = assessor(test_im)\n",
    "emonet_res['real_img'] = [s.detach().numpy()[0] for s in score]\n",
    "print(\"====Real Done!====\")\n"
   ],
   "id": "4cfa72d0176fc7ca",
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Unexpected type <class 'str'>",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[14], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m## real_img\u001B[39;00m\n\u001B[0;32m      2\u001B[0m image_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mC:\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mUsers\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124msOrOush\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mSoroushProjects\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124m14_CLIP_Ozcelic\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mresults\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124m00_Original_Images\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m----> 3\u001B[0m test_tensor \u001B[38;5;241m=\u001B[39m test_images_prep_assessor_CLIP(image_path)\n\u001B[0;32m      4\u001B[0m \u001B[38;5;66;03m# Reshape while keeping the last 3 dimensions\u001B[39;00m\n\u001B[0;32m      5\u001B[0m test_im \u001B[38;5;241m=\u001B[39m test_tensor\u001B[38;5;241m.\u001B[39mview(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m*\u001B[39mtest_tensor\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m3\u001B[39m:])\n",
      "Cell \u001B[1;32mIn[11], line 16\u001B[0m, in \u001B[0;36mtest_images_prep_assessor_CLIP\u001B[1;34m(images, target_size, mean)\u001B[0m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;66;03m# Iterate over each in-memory image\u001B[39;00m\n\u001B[0;32m     14\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m im \u001B[38;5;129;01min\u001B[39;00m images:\n\u001B[0;32m     15\u001B[0m     \u001B[38;5;66;03m# Apply the transformations (resize, mean subtraction, cropping, and to tensor)\u001B[39;00m\n\u001B[1;32m---> 16\u001B[0m     img_tensor \u001B[38;5;241m=\u001B[39m transform(im)\n\u001B[0;32m     17\u001B[0m     data_tensor\u001B[38;5;241m.\u001B[39mappend(img_tensor)\n\u001B[0;32m     19\u001B[0m \u001B[38;5;66;03m# Stack the list of tensors into a single tensor\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torchvision\\transforms\\transforms.py:95\u001B[0m, in \u001B[0;36mCompose.__call__\u001B[1;34m(self, img)\u001B[0m\n\u001B[0;32m     93\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, img):\n\u001B[0;32m     94\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransforms:\n\u001B[1;32m---> 95\u001B[0m         img \u001B[38;5;241m=\u001B[39m t(img)\n\u001B[0;32m     96\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m img\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torchvision\\transforms\\transforms.py:354\u001B[0m, in \u001B[0;36mResize.forward\u001B[1;34m(self, img)\u001B[0m\n\u001B[0;32m    346\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, img):\n\u001B[0;32m    347\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    348\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[0;32m    349\u001B[0m \u001B[38;5;124;03m        img (PIL Image or Tensor): Image to be scaled.\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    352\u001B[0m \u001B[38;5;124;03m        PIL Image or Tensor: Rescaled image.\u001B[39;00m\n\u001B[0;32m    353\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 354\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mresize(img, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msize, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minterpolation, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmax_size, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mantialias)\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torchvision\\transforms\\functional.py:465\u001B[0m, in \u001B[0;36mresize\u001B[1;34m(img, size, interpolation, max_size, antialias)\u001B[0m\n\u001B[0;32m    459\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m max_size \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(size) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m    460\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    461\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmax_size should only be passed if size specifies the length of the smaller edge, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    462\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mi.e. size should be an int or a sequence of length 1 in torchscript mode.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    463\u001B[0m         )\n\u001B[1;32m--> 465\u001B[0m _, image_height, image_width \u001B[38;5;241m=\u001B[39m get_dimensions(img)\n\u001B[0;32m    466\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(size, \u001B[38;5;28mint\u001B[39m):\n\u001B[0;32m    467\u001B[0m     size \u001B[38;5;241m=\u001B[39m [size]\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torchvision\\transforms\\functional.py:80\u001B[0m, in \u001B[0;36mget_dimensions\u001B[1;34m(img)\u001B[0m\n\u001B[0;32m     77\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(img, torch\u001B[38;5;241m.\u001B[39mTensor):\n\u001B[0;32m     78\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F_t\u001B[38;5;241m.\u001B[39mget_dimensions(img)\n\u001B[1;32m---> 80\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m F_pil\u001B[38;5;241m.\u001B[39mget_dimensions(img)\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torchvision\\transforms\\_functional_pil.py:31\u001B[0m, in \u001B[0;36mget_dimensions\u001B[1;34m(img)\u001B[0m\n\u001B[0;32m     29\u001B[0m     width, height \u001B[38;5;241m=\u001B[39m img\u001B[38;5;241m.\u001B[39msize\n\u001B[0;32m     30\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [channels, height, width]\n\u001B[1;32m---> 31\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnexpected type \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(img)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mTypeError\u001B[0m: Unexpected type <class 'str'>"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b7f385c528c63142"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
