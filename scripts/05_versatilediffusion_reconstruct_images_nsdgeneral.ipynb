{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-10T13:58:30.499373Z",
     "start_time": "2025-04-10T13:58:30.496123Z"
    }
   },
   "source": [
    "import sys\n",
    "sys.path.append(r'C:\\Users\\sOrOush\\SoroushProjects\\01_Soroush_and_Shakiba\\NSD_High_Dimensional_Data\\11_Marco_And_Soroush\\versatile_diffusion')"
   ],
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T13:58:33.702201Z",
     "start_time": "2025-04-10T13:58:33.700063Z"
    }
   },
   "cell_type": "code",
   "source": "#pip install scikit-image",
   "id": "a647658098bbe96e",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T13:58:33.715039Z",
     "start_time": "2025-04-10T13:58:33.711754Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "import PIL\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as tvtrans\n",
    "from lib.cfg_helper import model_cfg_bank\n",
    "from lib.model_zoo import get_model\n",
    "from lib.model_zoo.ddim_vd import DDIMSampler_VD\n",
    "from lib.experiments.sd_default import color_adjust, auto_merge_imlist\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from lib.model_zoo.vd import VD\n",
    "from lib.cfg_holder import cfg_unique_holder as cfguh\n",
    "from lib.cfg_helper import get_command_line_args, cfg_initiates, load_cfg_yaml\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.transform import resize, downscale_local_mean"
   ],
   "id": "ea90aa6ba2b7ef25",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T15:09:48.076931Z",
     "start_time": "2025-04-10T15:09:48.065352Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import argparse\n",
    "import sys\n",
    "\n",
    "# Check if the script is being run in an interactive environment\n",
    "if 'ipykernel' in sys.modules:\n",
    "    # Manually set the 'sub' variable\n",
    "    sub = 1  # You can change this value as needed\n",
    "else:\n",
    "    # Use argparse as usual\n",
    "    parser = argparse.ArgumentParser(description='Argument Parser')\n",
    "    parser.add_argument(\"-sub\", \"--sub\", help=\"Subject Number\", default=1, type=int)\n",
    "    args = parser.parse_args()\n",
    "    sub = args.sub\n",
    "\n",
    "# Ensure that sub is within the allowed range\n",
    "assert sub in [1, 2, 5, 7], \"The value of 'sub' must be one of [1, 2, 5, 7]\"\n",
    "\n",
    "print(f\"The value of 'sub' is: {sub}\")"
   ],
   "id": "7d2e2eacc79ff2c0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The value of 'sub' is: 1\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T13:58:36.861078Z",
     "start_time": "2025-04-10T13:58:36.853982Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def regularize_image(x):\n",
    "        BICUBIC = PIL.Image.Resampling.BICUBIC\n",
    "        if isinstance(x, str):\n",
    "            x = Image.open(x).resize([512, 512], resample=BICUBIC)\n",
    "            x = tvtrans.ToTensor()(x)\n",
    "        elif isinstance(x, PIL.Image.Image):\n",
    "            x = x.resize([512, 512], resample=BICUBIC)\n",
    "            x = tvtrans.ToTensor()(x)\n",
    "        elif isinstance(x, np.ndarray):\n",
    "            x = PIL.Image.fromarray(x).resize([512, 512], resample=BICUBIC)\n",
    "            x = tvtrans.ToTensor()(x)\n",
    "        elif isinstance(x, torch.Tensor):\n",
    "            pass\n",
    "        else:\n",
    "            assert False, 'Unknown image type'\n",
    "\n",
    "        assert (x.shape[1]==512) & (x.shape[2]==512), \\\n",
    "            'Wrong image size'\n",
    "        return x"
   ],
   "id": "7d3e1d976dd02d87",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T13:58:39.719950Z",
     "start_time": "2025-04-10T13:58:39.714549Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "os.chdir(r'C:\\Users\\sOrOush\\SoroushProjects\\01_Soroush_and_Shakiba\\NSD_High_Dimensional_Data\\11_Marco_And_Soroush')\n",
    "print(os.getcwd())  # Verify the new working directory"
   ],
   "id": "99d31db6e8d88194",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sOrOush\\SoroushProjects\\01_Soroush_and_Shakiba\\NSD_High_Dimensional_Data\\11_Marco_And_Soroush\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T15:12:14.609528Z",
     "start_time": "2025-04-10T15:11:50.868280Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cfgm_name = 'vd_noema'\n",
    "# sampler = DDIMSampler_VD\n",
    "pth = 'versatile_diffusion/pretrained/vd-four-flow-v1-0-fp16-deprecated.pth'\n",
    "cfgm = model_cfg_bank()(cfgm_name)\n",
    "net = get_model()(cfgm)\n",
    "sd = torch.load(pth, map_location='cpu')\n",
    "net.load_state_dict(sd, strict=False)"
   ],
   "id": "c703551420b76eaf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#######################\n",
      "# Running in eps mode #\n",
      "#######################\n",
      "\n",
      "Load openai_unet_2d with total 859520964 parameters,99512.246 parameter sum.\n",
      "Load openai_unet_0dmd with total 1708848448 parameters,249865.621 parameter sum.\n",
      "Load openai_unet_vd with total 2566318852 parameters,349365.192 parameter sum.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Load pth from versatile_diffusion/pretrained/kl-f8.pth\n",
      "Load autoencoderkl with total 83653863 parameters,72921.759 parameter sum.\n",
      "Load optimus_bert_connector with total 109489920 parameters,19490.449 parameter sum.\n",
      "Load optimus_gpt2_connector with total 132109824 parameters,18990.663 parameter sum.\n",
      "Load pth from versatile_diffusion/pretrained/optimus-vae.pth\n",
      "Load optimus_vae with total 241599744 parameters,-344611.688 parameter sum.\n",
      "Load clip_frozen with total 427616513 parameters,64007.510 parameter sum.\n",
      "Load vd with total 3319188972 parameters,141682.773 parameter sum.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sOrOush\\AppData\\Local\\Temp\\ipykernel_32344\\1952740532.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  sd = torch.load(pth, map_location='cpu')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=[], unexpected_keys=['clip.model.text_model.embeddings.position_ids', 'clip.model.vision_model.embeddings.position_ids'])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T13:57:21.276981Z",
     "start_time": "2025-04-10T13:57:21.272026Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f\"Number of available GPUs: {torch.cuda.device_count()}\")\n",
    "print(f\"Current CUDA device: {torch.cuda.current_device()}\")"
   ],
   "id": "87d6dca20f379b5e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of available GPUs: 1\n",
      "Current CUDA device: 0\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T13:57:24.370265Z",
     "start_time": "2025-04-10T13:57:24.364146Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ],
   "id": "20cc1bbd45584fbd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T13:57:28.681716Z",
     "start_time": "2025-04-10T13:57:28.676639Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check if CUDA is available\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "\n",
    "# Get the current device\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA Device Name:\", torch.cuda.get_device_name(0))\n",
    "    print(\"CUDA Device Count:\", torch.cuda.device_count())\n",
    "    print(\"Current CUDA Device:\", torch.cuda.current_device())\n",
    "else:\n",
    "    print(\"No CUDA device detected.\")"
   ],
   "id": "9d0af5018c98bd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "CUDA Device Name: NVIDIA GeForce RTX 4080 Laptop GPU\n",
      "CUDA Device Count: 1\n",
      "Current CUDA Device: 0\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T13:57:34.017046Z",
     "start_time": "2025-04-10T13:57:33.852965Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "b646b7b8864b8926",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T13:42:21.713323Z",
     "start_time": "2025-04-07T13:42:19.336627Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# GPU assignments\n",
    "net.clip.to(device)\n",
    "net.autokl.to(device)\n",
    "\n",
    "# If you uncomment these lines, make sure to use the correct device\n",
    "# net.model.to(device)\n",
    "sampler = DDIMSampler_VD(net)\n",
    "#sampler.model.model.cuda(1)\n",
    "#sampler.model.cuda(1)\n",
    "batch_size = 1  # You can adjust this based on your GPU memory\n",
    "\n",
    "\n",
    "base_path = r'C:\\Users\\sOrOush\\SoroushProjects\\14_CLIP_Ozcelic\\results'\n",
    "sub = 2  # replace with your subject index\n",
    "\n",
    "# Load the predicted features from files\n",
    "pred_text = np.load(os.path.join(base_path, f'predicted_features/subj{sub:02d}/nsd_cliptext_predtest_nsdgeneral.npy'))\n",
    "pred_vision = np.load(os.path.join(base_path, f'predicted_features/subj{sub:02d}/nsd_clipvision_predtest_nsdgeneral.npy'))\n",
    "\n",
    "# Try moving tensors to the GPU, and handle any errors that occur\n",
    "try:\n",
    "    pred_text = torch.tensor(pred_text).half().to(device)\n",
    "    pred_vision = torch.tensor(pred_vision).half().to(device)\n",
    "except RuntimeError as e:\n",
    "    print(f\"Error moving tensors to GPU: {e}\")\n",
    "    print(\"Attempting to use CPU instead...\")\n",
    "    device = torch.device(\"cpu\")\n",
    "    pred_text = torch.tensor(pred_text).half()\n",
    "    pred_vision = torch.tensor(pred_vision).half()\n"
   ],
   "id": "e25d85816f197dd7",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T15:30:07.807897Z",
     "start_time": "2025-04-10T15:30:07.752581Z"
    }
   },
   "cell_type": "code",
   "source": [
    "n_samples = 1\n",
    "ddim_steps = 50\n",
    "ddim_eta = 0\n",
    "scale = 7.5\n",
    "xtype = 'image'\n",
    "ctype = 'prompt'\n",
    "net.autokl.half()"
   ],
   "id": "76604bb111cf37fa",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AutoencoderKL(\n",
       "  (encoder): Encoder(\n",
       "    (conv_in): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (down): ModuleList(\n",
       "      (0): Module(\n",
       "        (block): ModuleList(\n",
       "          (0-1): 2 x ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (attn): ModuleList()\n",
       "        (downsample): Downsample(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2))\n",
       "        )\n",
       "      )\n",
       "      (1): Module(\n",
       "        (block): ModuleList(\n",
       "          (0): ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nin_shortcut): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (attn): ModuleList()\n",
       "        (downsample): Downsample(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))\n",
       "        )\n",
       "      )\n",
       "      (2): Module(\n",
       "        (block): ModuleList(\n",
       "          (0): ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nin_shortcut): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (attn): ModuleList()\n",
       "        (downsample): Downsample(\n",
       "          (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2))\n",
       "        )\n",
       "      )\n",
       "      (3): Module(\n",
       "        (block): ModuleList(\n",
       "          (0-1): 2 x ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (attn): ModuleList()\n",
       "      )\n",
       "    )\n",
       "    (mid): Module(\n",
       "      (block_1): ResnetBlock(\n",
       "        (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (attn_1): AttnBlock(\n",
       "        (norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "        (q): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (k): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (v): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (proj_out): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (block_2): ResnetBlock(\n",
       "        (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (norm_out): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "    (conv_out): Conv2d(512, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (conv_in): Conv2d(4, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (mid): Module(\n",
       "      (block_1): ResnetBlock(\n",
       "        (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (attn_1): AttnBlock(\n",
       "        (norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "        (q): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (k): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (v): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (proj_out): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (block_2): ResnetBlock(\n",
       "        (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (up): ModuleList(\n",
       "      (0): Module(\n",
       "        (block): ModuleList(\n",
       "          (0): ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nin_shortcut): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1-2): 2 x ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (attn): ModuleList()\n",
       "      )\n",
       "      (1): Module(\n",
       "        (block): ModuleList(\n",
       "          (0): ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nin_shortcut): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1-2): 2 x ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (attn): ModuleList()\n",
       "        (upsample): Upsample(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (2-3): 2 x Module(\n",
       "        (block): ModuleList(\n",
       "          (0-2): 3 x ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (attn): ModuleList()\n",
       "        (upsample): Upsample(\n",
       "          (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm_out): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "    (conv_out): Conv2d(128, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (quant_conv): Conv2d(8, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (post_quant_conv): Conv2d(4, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T15:30:12.643051Z",
     "start_time": "2025-04-10T15:30:12.638197Z"
    }
   },
   "cell_type": "code",
   "source": "print(os.getcwd())",
   "id": "45e232cc153df62b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sOrOush\\SoroushProjects\\01_Soroush_and_Shakiba\\NSD_High_Dimensional_Data\\11_Marco_And_Soroush\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T15:30:34.508342Z",
     "start_time": "2025-04-10T15:30:34.421733Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.manual_seed(0)\n",
    "# Loop through your image generation process, without saving to disk\n",
    "all_generated_images = []\n",
    "\n",
    "for im_id in range(len(pred_vision)):\n",
    "    # zim = Image.open(f'/media/SSD/04_Marco/03_fMRI_recon/generated images/VDVAE_regression_theta/{im_id}.png')\n",
    "    # zim = Image.open(f'/media/SSD/04_Marco/03_fMRI_recon/generated images/VDVAE/subj{sub:02d}/{im_id}.png') # <------- # original\n",
    "    # zim = Image.open(f'/media/SSD/04_Marco/03_fMRI_recon/generated images/VDVAE/negative_theta/{im_id}.png')# <-------- # negative theta\n",
    "    # zim = Image.open(f'/media/SSD/04_Marco/03_fMRI_recon/generated images/VDVAE/positive_theta/{im_id}.png') # <------- positive theta\n",
    "    zim = Image.open(f'C:/Users/sOrOush/SoroushProjects/14_CLIP_Ozcelic/results/generated images/VDVAE/nsdgeneral/degree1/subj01/{im_id}.png')\n",
    "    zim = regularize_image(zim)\n",
    "    zin = zim * 2 - 1\n",
    "    zin = zin.unsqueeze(0).to(device).half()\n",
    "\n",
    "    init_latent = net.autokl_encode(zin)\n",
    "\n",
    "    sampler.make_schedule(ddim_num_steps=ddim_steps, ddim_eta=ddim_eta, verbose=False)\n",
    "    strength = strength_var\n",
    "    assert 0. <= strength <= 1., 'can only work with strength in [0.0, 1.0]'\n",
    "    t_enc = int(strength * ddim_steps)\n",
    "\n",
    "    z_enc = sampler.stochastic_encode(init_latent, torch.tensor([t_enc]).to(device))\n",
    "\n",
    "    dummy = ''\n",
    "    utx = net.clip_encode_text(dummy).to(device).half()\n",
    "\n",
    "    dummy = torch.zeros((1, 3, 224, 224)).to(device)\n",
    "    uim = net.clip_encode_vision(dummy).to(device).half()\n",
    "\n",
    "    z_enc = z_enc.to(device).half()\n",
    "\n",
    "    h, w = 512, 512\n",
    "    shape = [n_samples, 4, h // 8, w // 8]\n",
    "\n",
    "    cim = pred_vision[im_id].unsqueeze(0).to(device)\n",
    "    ctx = pred_text[im_id].unsqueeze(0).to(device)\n",
    "\n",
    "\n",
    "    sampler.model.model.diffusion_model.device = device\n",
    "    sampler.model.model.diffusion_model.half().to(device)\n",
    "    mixing = mixing_var\n",
    "    z = sampler.decode_dc(\n",
    "        x_latent=z_enc,\n",
    "        first_conditioning=[uim, cim],\n",
    "        second_conditioning=[utx, ctx],\n",
    "        t_start=t_enc,\n",
    "        unconditional_guidance_scale=scale,\n",
    "        xtype='image',\n",
    "        first_ctype='vision',\n",
    "        second_ctype='prompt',\n",
    "        mixed_ratio=(1-mixing),\n",
    "    )\n",
    "\n",
    "    z = z.to(device).half()\n",
    "    x = net.autokl_decode(z)\n",
    "    color_adj='None'\n",
    "    color_adj_flag = (color_adj.lower() != 'none') and (color_adj is not None)\n",
    "    color_adj_simple = (color_adj.lower() == 'simple')\n",
    "    color_adj_keep_ratio = 0.5\n",
    "\n",
    "    if color_adj_flag and (ctype == 'vision'):\n",
    "        x_adj = []\n",
    "        for xi in x:\n",
    "            color_adj_f = color_adjust(ref_from=(xi + 1) / 2, ref_to=color_adj_to)\n",
    "            xi_adj = color_adj_f((xi + 1) / 2, keep=color_adj_keep_ratio, simple=color_adj_simple)\n",
    "            x_adj.append(xi_adj)\n",
    "        x = x_adj\n",
    "    else:\n",
    "        x = torch.clamp((x + 1.0) / 2.0, min=0.0, max=1.0)\n",
    "        x = [tvtrans.ToPILImage()(xi) for xi in x]\n",
    "\n",
    "    # Add generated image to the list\n",
    "    x[0].save(f'C:/Users/sOrOush/SoroushProjects/14_CLIP_Ozcelic/results/generated_images/versatile_diffusion/nsdgeneral/degree1/subj01/{im_id}.png')\n",
    "    # x[0].save(f'/media/SSD/04_Marco/03_fMRI_recon/generated images/CLIP/emonet/positve_theta/{im_id}.png')\n",
    "    # x[0].save(f'/media/SSD/04_Marco/03_fMRI_recon/generated images/CLIP/emonet/negative_theta/{im_id}.png')\n",
    "    # x[0].save(f'/media/SSD/04_Marco/03_fMRI_recon/generated images/CLIP/original/{im_id}.png')\n",
    "    # x[0].save(f'/media/SSD/04_Marco/03_fMRI_recon/generated images/CLIP_regression_theta/{im_id}.png')\n"
   ],
   "id": "f2d2bb424ca9c86b",
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.cuda.HalfTensor) and weight type (torch.HalfTensor) should be the same",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[45], line 15\u001B[0m\n\u001B[0;32m     12\u001B[0m zin \u001B[38;5;241m=\u001B[39m zim \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m2\u001B[39m \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m     13\u001B[0m zin \u001B[38;5;241m=\u001B[39m zin\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m0\u001B[39m)\u001B[38;5;241m.\u001B[39mto(device)\u001B[38;5;241m.\u001B[39mhalf()\n\u001B[1;32m---> 15\u001B[0m init_latent \u001B[38;5;241m=\u001B[39m net\u001B[38;5;241m.\u001B[39mautokl_encode(zin)\n\u001B[0;32m     17\u001B[0m sampler\u001B[38;5;241m.\u001B[39mmake_schedule(ddim_num_steps\u001B[38;5;241m=\u001B[39mddim_steps, ddim_eta\u001B[38;5;241m=\u001B[39mddim_eta, verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m     18\u001B[0m strength \u001B[38;5;241m=\u001B[39m strength_var\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\_contextlib.py:116\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    113\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[0;32m    114\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m    115\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[1;32m--> 116\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\SoroushProjects\\01_Soroush_and_Shakiba\\NSD_High_Dimensional_Data\\11_Marco_And_Soroush\\versatile_diffusion\\lib\\model_zoo\\vd.py:294\u001B[0m, in \u001B[0;36mVD.autokl_encode\u001B[1;34m(self, image)\u001B[0m\n\u001B[0;32m    292\u001B[0m \u001B[38;5;129m@torch\u001B[39m\u001B[38;5;241m.\u001B[39mno_grad()\n\u001B[0;32m    293\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mautokl_encode\u001B[39m(\u001B[38;5;28mself\u001B[39m, image):\n\u001B[1;32m--> 294\u001B[0m     encoder_posterior \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mautokl\u001B[38;5;241m.\u001B[39mencode(image)\n\u001B[0;32m    295\u001B[0m     z \u001B[38;5;241m=\u001B[39m encoder_posterior\u001B[38;5;241m.\u001B[39msample()\n\u001B[0;32m    296\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mscale_factor \u001B[38;5;241m*\u001B[39m z\n",
      "File \u001B[1;32m~\\SoroushProjects\\01_Soroush_and_Shakiba\\NSD_High_Dimensional_Data\\11_Marco_And_Soroush\\versatile_diffusion\\lib\\model_zoo\\autoencoder.py:308\u001B[0m, in \u001B[0;36mAutoencoderKL.encode\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m    307\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mencode\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[1;32m--> 308\u001B[0m     h \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mencoder(x)\n\u001B[0;32m    309\u001B[0m     moments \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mquant_conv(h)\n\u001B[0;32m    310\u001B[0m     posterior \u001B[38;5;241m=\u001B[39m DiagonalGaussianDistribution(moments)\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32m~\\SoroushProjects\\01_Soroush_and_Shakiba\\NSD_High_Dimensional_Data\\11_Marco_And_Soroush\\versatile_diffusion\\lib\\model_zoo\\diffusion_modules.py:439\u001B[0m, in \u001B[0;36mEncoder.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m    436\u001B[0m temb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    438\u001B[0m \u001B[38;5;66;03m# downsampling\u001B[39;00m\n\u001B[1;32m--> 439\u001B[0m hs \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconv_in(x)]\n\u001B[0;32m    440\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i_level \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_resolutions):\n\u001B[0;32m    441\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i_block \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_res_blocks):\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\conv.py:554\u001B[0m, in \u001B[0;36mConv2d.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    553\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 554\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_conv_forward(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mweight, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbias)\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\conv.py:549\u001B[0m, in \u001B[0;36mConv2d._conv_forward\u001B[1;34m(self, input, weight, bias)\u001B[0m\n\u001B[0;32m    537\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mzeros\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m    538\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mconv2d(\n\u001B[0;32m    539\u001B[0m         F\u001B[38;5;241m.\u001B[39mpad(\n\u001B[0;32m    540\u001B[0m             \u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reversed_padding_repeated_twice, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    547\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgroups,\n\u001B[0;32m    548\u001B[0m     )\n\u001B[1;32m--> 549\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mconv2d(\n\u001B[0;32m    550\u001B[0m     \u001B[38;5;28minput\u001B[39m, weight, bias, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstride, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdilation, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgroups\n\u001B[0;32m    551\u001B[0m )\n",
      "\u001B[1;31mRuntimeError\u001B[0m: Input type (torch.cuda.HalfTensor) and weight type (torch.HalfTensor) should be the same"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T13:48:46.123131Z",
     "start_time": "2025-04-07T13:47:39.152047Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torchvision.transforms as tvtrans\n",
    "import torch\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Paths\n",
    "input_dir = r'C:\\Users\\sOrOush\\SoroushProjects\\14_CLIP_Ozcelic\\data\\generated_images_predicted_35l\\subj02'\n",
    "save_dir = r'C:\\Users\\sOrOush\\SoroushProjects\\14_CLIP_Ozcelic\\results\\generated_images\\versatile_diffusion\\subj02\\generated_clip_images_predicted_35l'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# List all PNG files in the input folder\n",
    "image_filenames = sorted([f for f in os.listdir(input_dir) if f.endswith('.png')])\n",
    "\n",
    "for im_id, filename in enumerate(image_filenames):\n",
    "    image_path = os.path.join(input_dir, filename)\n",
    "\n",
    "    # Load and preprocess image\n",
    "    zim = Image.open(image_path)\n",
    "    zim = regularize_image(zim)\n",
    "    zin = zim * 2 - 1\n",
    "    zin = zin.unsqueeze(0).to(device).half()\n",
    "\n",
    "    # Encode image\n",
    "    init_latent = net.autokl_encode(zin)\n",
    "\n",
    "    # Sampling schedule\n",
    "    sampler.make_schedule(ddim_num_steps=ddim_steps, ddim_eta=ddim_eta, verbose=False)\n",
    "    strength = 0.75\n",
    "    t_enc = int(strength * ddim_steps)\n",
    "\n",
    "    # Add noise to latent\n",
    "    z_enc = sampler.stochastic_encode(init_latent, torch.tensor([t_enc]).to(device))\n",
    "\n",
    "    # Dummy conditioning\n",
    "    dummy_text = ''\n",
    "    utx = net.clip_encode_text(dummy_text).to(device).half()\n",
    "\n",
    "    dummy_vision = torch.zeros((1, 3, 224, 224)).to(device)\n",
    "    uim = net.clip_encode_vision(dummy_vision).to(device).half()\n",
    "\n",
    "    # Get context from pred_vision/text\n",
    "    cim = pred_vision[im_id].unsqueeze(0).to(device)\n",
    "    ctx = pred_text[im_id].unsqueeze(0).to(device)\n",
    "\n",
    "    # Ensure model is on correct device\n",
    "    sampler.model.model.diffusion_model.device = device\n",
    "    sampler.model.model.diffusion_model.half().to(device)\n",
    "\n",
    "    # Decode image\n",
    "    mixing = 0.4\n",
    "    z = sampler.decode_dc(\n",
    "        x_latent=z_enc,\n",
    "        first_conditioning=[uim, cim],\n",
    "        second_conditioning=[utx, ctx],\n",
    "        t_start=t_enc,\n",
    "        unconditional_guidance_scale=scale,\n",
    "        xtype='image',\n",
    "        first_ctype='vision',\n",
    "        second_ctype='prompt',\n",
    "        mixed_ratio=(1 - mixing),\n",
    "    )\n",
    "\n",
    "    # Decode and post-process\n",
    "    z = z.to(device).half()\n",
    "    x = net.autokl_decode(z)\n",
    "\n",
    "    color_adj = 'None'\n",
    "    color_adj_flag = (color_adj.lower() != 'none') and (color_adj is not None)\n",
    "    color_adj_simple = (color_adj.lower() == 'simple')\n",
    "    color_adj_keep_ratio = 0.5\n",
    "\n",
    "    if color_adj_flag and (ctype == 'vision'):\n",
    "        x_adj = []\n",
    "        for xi in x:\n",
    "            color_adj_f = color_adjust(ref_from=(xi + 1) / 2, ref_to=color_adj_to)\n",
    "            xi_adj = color_adj_f((xi + 1) / 2, keep=color_adj_keep_ratio, simple=color_adj_simple)\n",
    "            x_adj.append(xi_adj)\n",
    "        x = x_adj\n",
    "    else:\n",
    "        x = torch.clamp((x + 1.0) / 2.0, min=0.0, max=1.0)\n",
    "        x = [tvtrans.ToPILImage()(xi) for xi in x]\n",
    "\n",
    "    # Save image using original filename\n",
    "    save_path = os.path.join(save_dir, filename)\n",
    "    x[0].save(save_path)\n"
   ],
   "id": "356d086b136e7ea9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 37 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 37/37 [00:05<00:00,  6.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 37 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 37/37 [00:05<00:00,  6.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 37 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 37/37 [00:05<00:00,  6.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 37 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 37/37 [00:05<00:00,  6.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 37 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 37/37 [00:05<00:00,  6.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 37 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 37/37 [00:05<00:00,  6.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 37 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 37/37 [00:05<00:00,  6.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 37 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 37/37 [00:05<00:00,  6.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 37 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 37/37 [00:05<00:00,  6.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 37 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 37/37 [00:05<00:00,  6.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 37 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 37/37 [00:05<00:00,  6.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 37 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image:   5%|▌         | 2/37 [00:00<00:08,  4.33it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[16], line 53\u001B[0m\n\u001B[0;32m     51\u001B[0m \u001B[38;5;66;03m# Decode image\u001B[39;00m\n\u001B[0;32m     52\u001B[0m mixing \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.4\u001B[39m\n\u001B[1;32m---> 53\u001B[0m z \u001B[38;5;241m=\u001B[39m sampler\u001B[38;5;241m.\u001B[39mdecode_dc(\n\u001B[0;32m     54\u001B[0m     x_latent\u001B[38;5;241m=\u001B[39mz_enc,\n\u001B[0;32m     55\u001B[0m     first_conditioning\u001B[38;5;241m=\u001B[39m[uim, cim],\n\u001B[0;32m     56\u001B[0m     second_conditioning\u001B[38;5;241m=\u001B[39m[utx, ctx],\n\u001B[0;32m     57\u001B[0m     t_start\u001B[38;5;241m=\u001B[39mt_enc,\n\u001B[0;32m     58\u001B[0m     unconditional_guidance_scale\u001B[38;5;241m=\u001B[39mscale,\n\u001B[0;32m     59\u001B[0m     xtype\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mimage\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m     60\u001B[0m     first_ctype\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mvision\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m     61\u001B[0m     second_ctype\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mprompt\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m     62\u001B[0m     mixed_ratio\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m-\u001B[39m mixing),\n\u001B[0;32m     63\u001B[0m )\n\u001B[0;32m     65\u001B[0m \u001B[38;5;66;03m# Decode and post-process\u001B[39;00m\n\u001B[0;32m     66\u001B[0m z \u001B[38;5;241m=\u001B[39m z\u001B[38;5;241m.\u001B[39mto(device)\u001B[38;5;241m.\u001B[39mhalf()\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\_contextlib.py:116\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    113\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[0;32m    114\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m    115\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[1;32m--> 116\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\SoroushProjects\\01_Soroush_and_Shakiba\\NSD_High_Dimensional_Data\\11_Marco_And_Soroush\\versatile_diffusion\\lib\\model_zoo\\ddim_vd.py:399\u001B[0m, in \u001B[0;36mDDIMSampler_VD.decode_dc\u001B[1;34m(self, x_latent, first_conditioning, second_conditioning, t_start, unconditional_guidance_scale, unconditional_conditioning, xtype, first_ctype, second_ctype, use_original_steps, mixed_ratio, callback)\u001B[0m\n\u001B[0;32m    397\u001B[0m     index \u001B[38;5;241m=\u001B[39m total_steps \u001B[38;5;241m-\u001B[39m i \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    398\u001B[0m     ts \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mfull((x_latent\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m],), step, device\u001B[38;5;241m=\u001B[39mx_latent\u001B[38;5;241m.\u001B[39mdevice, dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mlong)\n\u001B[1;32m--> 399\u001B[0m     x_dec, _ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mp_sample_ddim_dc(\n\u001B[0;32m    400\u001B[0m         x_dec, \n\u001B[0;32m    401\u001B[0m         first_conditioning, \n\u001B[0;32m    402\u001B[0m         second_conditioning, \n\u001B[0;32m    403\u001B[0m         ts, index, \n\u001B[0;32m    404\u001B[0m         unconditional_guidance_scale\u001B[38;5;241m=\u001B[39munconditional_guidance_scale,\n\u001B[0;32m    405\u001B[0m         xtype\u001B[38;5;241m=\u001B[39mxtype,\n\u001B[0;32m    406\u001B[0m         first_ctype\u001B[38;5;241m=\u001B[39mfirst_ctype,\n\u001B[0;32m    407\u001B[0m         second_ctype\u001B[38;5;241m=\u001B[39msecond_ctype,\n\u001B[0;32m    408\u001B[0m         use_original_steps\u001B[38;5;241m=\u001B[39muse_original_steps,\n\u001B[0;32m    409\u001B[0m         noise_dropout\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m,\n\u001B[0;32m    410\u001B[0m         temperature\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m,\n\u001B[0;32m    411\u001B[0m         mixed_ratio\u001B[38;5;241m=\u001B[39mmixed_ratio,)\n\u001B[0;32m    412\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m callback: callback(i)\n\u001B[0;32m    413\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m x_dec\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\_contextlib.py:116\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    113\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[0;32m    114\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m    115\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[1;32m--> 116\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\SoroushProjects\\01_Soroush_and_Shakiba\\NSD_High_Dimensional_Data\\11_Marco_And_Soroush\\versatile_diffusion\\lib\\model_zoo\\ddim_vd.py:278\u001B[0m, in \u001B[0;36mDDIMSampler_VD.p_sample_ddim_dc\u001B[1;34m(self, x, first_conditioning, second_conditioning, t, index, unconditional_guidance_scale, xtype, first_ctype, second_ctype, repeat_noise, use_original_steps, noise_dropout, temperature, mixed_ratio)\u001B[0m\n\u001B[0;32m    275\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m xtype \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtext\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[0;32m    276\u001B[0m     extended_shape \u001B[38;5;241m=\u001B[39m (b, \u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m--> 278\u001B[0m a_t \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mfull(extended_shape, alphas[index], device\u001B[38;5;241m=\u001B[39mdevice, dtype\u001B[38;5;241m=\u001B[39mx\u001B[38;5;241m.\u001B[39mdtype)\n\u001B[0;32m    279\u001B[0m a_prev \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mfull(extended_shape, alphas_prev[index], device\u001B[38;5;241m=\u001B[39mdevice, dtype\u001B[38;5;241m=\u001B[39mx\u001B[38;5;241m.\u001B[39mdtype)\n\u001B[0;32m    280\u001B[0m sigma_t \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mfull(extended_shape, sigmas[index], device\u001B[38;5;241m=\u001B[39mdevice, dtype\u001B[38;5;241m=\u001B[39mx\u001B[38;5;241m.\u001B[39mdtype)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T14:07:28.090352Z",
     "start_time": "2025-04-07T14:05:54.541906Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import re\n",
    "from PIL import Image\n",
    "import torchvision.transforms as tvtrans\n",
    "import torch\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Natural sort helper for filenames\n",
    "def extract_number(filename):\n",
    "    match = re.search(r'(\\d+)', filename)\n",
    "    return int(match.group(1)) if match else -1\n",
    "\n",
    "# === Paths ===\n",
    "input_dir = r'C:\\Users\\sOrOush\\SoroushProjects\\14_CLIP_Ozcelic\\data\\generated_images_predicted_35l\\subj02'\n",
    "save_dir = r'C:\\Users\\sOrOush\\SoroushProjects\\14_CLIP_Ozcelic\\results\\generated_images\\versatile_diffusion\\subj02\\generated_clip_images_predicted_35l'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Get naturally sorted filenames\n",
    "image_filenames = sorted(\n",
    "    [f for f in os.listdir(input_dir) if f.endswith('.png')],\n",
    "    key=extract_number\n",
    ")\n",
    "\n",
    "# === Main loop ===\n",
    "for im_id in range(len(pred_vision)):\n",
    "\n",
    "    image_path = os.path.join(input_dir, image_filenames[im_id])\n",
    "    zim = Image.open(image_path)\n",
    "    zim = regularize_image(zim)\n",
    "    zin = zim * 2 - 1\n",
    "    zin = zin.unsqueeze(0).to(device).half()\n",
    "\n",
    "    init_latent = net.autokl_encode(zin)\n",
    "\n",
    "    sampler.make_schedule(ddim_num_steps=ddim_steps, ddim_eta=ddim_eta, verbose=False)\n",
    "    strength = 0.75\n",
    "    assert 0. <= strength <= 1., 'can only work with strength in [0.0, 1.0]'\n",
    "    t_enc = int(strength * ddim_steps)\n",
    "\n",
    "    z_enc = sampler.stochastic_encode(init_latent, torch.tensor([t_enc]).to(device))\n",
    "\n",
    "    dummy = ''\n",
    "    utx = net.clip_encode_text(dummy).to(device).half()\n",
    "\n",
    "    dummy = torch.zeros((1, 3, 224, 224)).to(device)\n",
    "    uim = net.clip_encode_vision(dummy).to(device).half()\n",
    "\n",
    "    z_enc = z_enc.to(device).half()\n",
    "\n",
    "    h, w = 512, 512\n",
    "    shape = [n_samples, 4, h // 8, w // 8]\n",
    "\n",
    "    cim = pred_vision[im_id].unsqueeze(0).to(device)\n",
    "    ctx = pred_text[im_id].unsqueeze(0).to(device)\n",
    "\n",
    "    sampler.model.model.diffusion_model.device = device\n",
    "    sampler.model.model.diffusion_model.half().to(device)\n",
    "\n",
    "    mixing = 0.4\n",
    "    z = sampler.decode_dc(\n",
    "        x_latent=z_enc,\n",
    "        first_conditioning=[uim, cim],\n",
    "        second_conditioning=[utx, ctx],\n",
    "        t_start=t_enc,\n",
    "        unconditional_guidance_scale=scale,\n",
    "        xtype='image',\n",
    "        first_ctype='vision',\n",
    "        second_ctype='prompt',\n",
    "        mixed_ratio=(1 - mixing),\n",
    "    )\n",
    "\n",
    "    z = z.to(device).half()\n",
    "    x = net.autokl_decode(z)\n",
    "\n",
    "    color_adj = 'None'\n",
    "    color_adj_flag = (color_adj.lower() != 'none') and (color_adj is not None)\n",
    "    color_adj_simple = (color_adj.lower() == 'simple')\n",
    "    color_adj_keep_ratio = 0.5\n",
    "\n",
    "    if color_adj_flag and (ctype == 'vision'):\n",
    "        x_adj = []\n",
    "        for xi in x:\n",
    "            color_adj_f = color_adjust(ref_from=(xi + 1) / 2, ref_to=color_adj_to)\n",
    "            xi_adj = color_adj_f((xi + 1) / 2, keep=color_adj_keep_ratio, simple=color_adj_simple)\n",
    "            x_adj.append(xi_adj)\n",
    "        x = x_adj\n",
    "    else:\n",
    "        x = torch.clamp((x + 1.0) / 2.0, min=0.0, max=1.0)\n",
    "        x = [tvtrans.ToPILImage()(xi) for xi in x]\n",
    "\n",
    "    # Save with same filename as input\n",
    "    save_path = os.path.join(save_dir, image_filenames[im_id])\n",
    "    x[0].save(save_path)\n"
   ],
   "id": "a216477375747d86",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 37 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 37/37 [00:05<00:00,  6.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 37 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 37/37 [00:05<00:00,  6.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 37 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 37/37 [00:05<00:00,  6.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 37 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 37/37 [00:05<00:00,  6.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 37 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 37/37 [00:05<00:00,  6.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 37 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 37/37 [00:05<00:00,  6.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 37 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 37/37 [00:05<00:00,  6.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 37 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 37/37 [00:05<00:00,  6.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 37 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 37/37 [00:06<00:00,  6.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 37 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 37/37 [00:05<00:00,  6.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 37 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 37/37 [00:05<00:00,  6.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 37 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 37/37 [00:06<00:00,  5.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 37 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 37/37 [00:06<00:00,  5.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 37 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 37/37 [00:06<00:00,  6.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 37 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image:  86%|████████▋ | 32/37 [00:05<00:00,  5.99it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[18], line 61\u001B[0m\n\u001B[0;32m     58\u001B[0m sampler\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mdiffusion_model\u001B[38;5;241m.\u001B[39mhalf()\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m     60\u001B[0m mixing \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.4\u001B[39m\n\u001B[1;32m---> 61\u001B[0m z \u001B[38;5;241m=\u001B[39m sampler\u001B[38;5;241m.\u001B[39mdecode_dc(\n\u001B[0;32m     62\u001B[0m     x_latent\u001B[38;5;241m=\u001B[39mz_enc,\n\u001B[0;32m     63\u001B[0m     first_conditioning\u001B[38;5;241m=\u001B[39m[uim, cim],\n\u001B[0;32m     64\u001B[0m     second_conditioning\u001B[38;5;241m=\u001B[39m[utx, ctx],\n\u001B[0;32m     65\u001B[0m     t_start\u001B[38;5;241m=\u001B[39mt_enc,\n\u001B[0;32m     66\u001B[0m     unconditional_guidance_scale\u001B[38;5;241m=\u001B[39mscale,\n\u001B[0;32m     67\u001B[0m     xtype\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mimage\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m     68\u001B[0m     first_ctype\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mvision\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m     69\u001B[0m     second_ctype\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mprompt\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m     70\u001B[0m     mixed_ratio\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m-\u001B[39m mixing),\n\u001B[0;32m     71\u001B[0m )\n\u001B[0;32m     73\u001B[0m z \u001B[38;5;241m=\u001B[39m z\u001B[38;5;241m.\u001B[39mto(device)\u001B[38;5;241m.\u001B[39mhalf()\n\u001B[0;32m     74\u001B[0m x \u001B[38;5;241m=\u001B[39m net\u001B[38;5;241m.\u001B[39mautokl_decode(z)\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\_contextlib.py:116\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    113\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[0;32m    114\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m    115\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[1;32m--> 116\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\SoroushProjects\\01_Soroush_and_Shakiba\\NSD_High_Dimensional_Data\\11_Marco_And_Soroush\\versatile_diffusion\\lib\\model_zoo\\ddim_vd.py:399\u001B[0m, in \u001B[0;36mDDIMSampler_VD.decode_dc\u001B[1;34m(self, x_latent, first_conditioning, second_conditioning, t_start, unconditional_guidance_scale, unconditional_conditioning, xtype, first_ctype, second_ctype, use_original_steps, mixed_ratio, callback)\u001B[0m\n\u001B[0;32m    397\u001B[0m     index \u001B[38;5;241m=\u001B[39m total_steps \u001B[38;5;241m-\u001B[39m i \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    398\u001B[0m     ts \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mfull((x_latent\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m],), step, device\u001B[38;5;241m=\u001B[39mx_latent\u001B[38;5;241m.\u001B[39mdevice, dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mlong)\n\u001B[1;32m--> 399\u001B[0m     x_dec, _ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mp_sample_ddim_dc(\n\u001B[0;32m    400\u001B[0m         x_dec, \n\u001B[0;32m    401\u001B[0m         first_conditioning, \n\u001B[0;32m    402\u001B[0m         second_conditioning, \n\u001B[0;32m    403\u001B[0m         ts, index, \n\u001B[0;32m    404\u001B[0m         unconditional_guidance_scale\u001B[38;5;241m=\u001B[39munconditional_guidance_scale,\n\u001B[0;32m    405\u001B[0m         xtype\u001B[38;5;241m=\u001B[39mxtype,\n\u001B[0;32m    406\u001B[0m         first_ctype\u001B[38;5;241m=\u001B[39mfirst_ctype,\n\u001B[0;32m    407\u001B[0m         second_ctype\u001B[38;5;241m=\u001B[39msecond_ctype,\n\u001B[0;32m    408\u001B[0m         use_original_steps\u001B[38;5;241m=\u001B[39muse_original_steps,\n\u001B[0;32m    409\u001B[0m         noise_dropout\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m,\n\u001B[0;32m    410\u001B[0m         temperature\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m,\n\u001B[0;32m    411\u001B[0m         mixed_ratio\u001B[38;5;241m=\u001B[39mmixed_ratio,)\n\u001B[0;32m    412\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m callback: callback(i)\n\u001B[0;32m    413\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m x_dec\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\_contextlib.py:116\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    113\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[0;32m    114\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m    115\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[1;32m--> 116\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\SoroushProjects\\01_Soroush_and_Shakiba\\NSD_High_Dimensional_Data\\11_Marco_And_Soroush\\versatile_diffusion\\lib\\model_zoo\\ddim_vd.py:262\u001B[0m, in \u001B[0;36mDDIMSampler_VD.p_sample_ddim_dc\u001B[1;34m(self, x, first_conditioning, second_conditioning, t, index, unconditional_guidance_scale, xtype, first_ctype, second_ctype, repeat_noise, use_original_steps, noise_dropout, temperature, mixed_ratio)\u001B[0m\n\u001B[0;32m    259\u001B[0m first_c \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcat(first_conditioning)\n\u001B[0;32m    260\u001B[0m second_c \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcat(second_conditioning)\n\u001B[1;32m--> 262\u001B[0m e_t_uncond, e_t \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mapply_model_dc(\n\u001B[0;32m    263\u001B[0m     x_in, t_in, first_c, second_c, xtype\u001B[38;5;241m=\u001B[39mxtype, first_ctype\u001B[38;5;241m=\u001B[39mfirst_ctype, second_ctype\u001B[38;5;241m=\u001B[39msecond_ctype, mixed_ratio\u001B[38;5;241m=\u001B[39mmixed_ratio)\u001B[38;5;241m.\u001B[39mchunk(\u001B[38;5;241m2\u001B[39m)\n\u001B[0;32m    265\u001B[0m e_t \u001B[38;5;241m=\u001B[39m e_t_uncond \u001B[38;5;241m+\u001B[39m unconditional_guidance_scale \u001B[38;5;241m*\u001B[39m (e_t \u001B[38;5;241m-\u001B[39m e_t_uncond)\n\u001B[0;32m    267\u001B[0m alphas \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39malphas_cumprod \u001B[38;5;28;01mif\u001B[39;00m use_original_steps \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mddim_alphas\n",
      "File \u001B[1;32m~\\SoroushProjects\\01_Soroush_and_Shakiba\\NSD_High_Dimensional_Data\\11_Marco_And_Soroush\\versatile_diffusion\\lib\\model_zoo\\vd.py:442\u001B[0m, in \u001B[0;36mVD.apply_model_dc\u001B[1;34m(self, x_noisy, t, first_c, second_c, xtype, first_ctype, second_ctype, mixed_ratio)\u001B[0m\n\u001B[0;32m    441\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mapply_model_dc\u001B[39m(\u001B[38;5;28mself\u001B[39m, x_noisy, t, first_c, second_c, xtype\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mimage\u001B[39m\u001B[38;5;124m'\u001B[39m, first_ctype\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mvision\u001B[39m\u001B[38;5;124m'\u001B[39m, second_ctype\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mprompt\u001B[39m\u001B[38;5;124m'\u001B[39m, mixed_ratio\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.5\u001B[39m):\n\u001B[1;32m--> 442\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mdiffusion_model\u001B[38;5;241m.\u001B[39mforward_dc(x_noisy, t, first_c, second_c, xtype, first_ctype, second_ctype, mixed_ratio)\n",
      "File \u001B[1;32m~\\SoroushProjects\\01_Soroush_and_Shakiba\\NSD_High_Dimensional_Data\\11_Marco_And_Soroush\\versatile_diffusion\\lib\\model_zoo\\openaimodel.py:2545\u001B[0m, in \u001B[0;36mUNetModelVD.forward_dc\u001B[1;34m(self, x, timesteps, c0, c1, xtype, c0_type, c1_type, mixed_ratio)\u001B[0m\n\u001B[0;32m   2543\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i_module, t_module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39munet_image\u001B[38;5;241m.\u001B[39moutput_blocks, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39munet_text\u001B[38;5;241m.\u001B[39moutput_blocks):\n\u001B[0;32m   2544\u001B[0m     h \u001B[38;5;241m=\u001B[39m th\u001B[38;5;241m.\u001B[39mcat([h, hs\u001B[38;5;241m.\u001B[39mpop()], dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m-> 2545\u001B[0m     h \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmixed_run_dc(i_module, t_module, h, emb, c0, c1, xtype, c0_type, c1_type, mixed_ratio)\n\u001B[0;32m   2546\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m xtype \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mimage\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[0;32m   2547\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39munet_image\u001B[38;5;241m.\u001B[39mout(h)\n",
      "File \u001B[1;32m~\\SoroushProjects\\01_Soroush_and_Shakiba\\NSD_High_Dimensional_Data\\11_Marco_And_Soroush\\versatile_diffusion\\lib\\model_zoo\\openaimodel.py:2560\u001B[0m, in \u001B[0;36mUNetModelVD.mixed_run_dc\u001B[1;34m(self, inet, tnet, x, emb, c0, c1, xtype, c0_type, c1_type, mixed_ratio)\u001B[0m\n\u001B[0;32m   2558\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(ilayer, SpatialTransformer):\n\u001B[0;32m   2559\u001B[0m     h0 \u001B[38;5;241m=\u001B[39m ilayer(h, c0)\u001B[38;5;241m-\u001B[39mh \u001B[38;5;28;01mif\u001B[39;00m c0_type\u001B[38;5;241m==\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mvision\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m tlayer(h, c0)\u001B[38;5;241m-\u001B[39mh\n\u001B[1;32m-> 2560\u001B[0m     h1 \u001B[38;5;241m=\u001B[39m ilayer(h, c1)\u001B[38;5;241m-\u001B[39mh \u001B[38;5;28;01mif\u001B[39;00m c1_type\u001B[38;5;241m==\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mvision\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m tlayer(h, c1)\u001B[38;5;241m-\u001B[39mh\n\u001B[0;32m   2561\u001B[0m     h \u001B[38;5;241m=\u001B[39m h0\u001B[38;5;241m*\u001B[39mmixed_ratio \u001B[38;5;241m+\u001B[39m h1\u001B[38;5;241m*\u001B[39m(\u001B[38;5;241m1\u001B[39m\u001B[38;5;241m-\u001B[39mmixed_ratio) \u001B[38;5;241m+\u001B[39m h\n\u001B[0;32m   2562\u001B[0m     \u001B[38;5;66;03m# h = ilayer(h, c0)\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32m~\\SoroushProjects\\01_Soroush_and_Shakiba\\NSD_High_Dimensional_Data\\11_Marco_And_Soroush\\versatile_diffusion\\lib\\model_zoo\\attention.py:263\u001B[0m, in \u001B[0;36mSpatialTransformer.forward\u001B[1;34m(self, x, context)\u001B[0m\n\u001B[0;32m    261\u001B[0m x \u001B[38;5;241m=\u001B[39m rearrange(x, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mb c h w -> b (h w) c\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39mcontiguous()\n\u001B[0;32m    262\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m block \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransformer_blocks:\n\u001B[1;32m--> 263\u001B[0m     x \u001B[38;5;241m=\u001B[39m block(x, context\u001B[38;5;241m=\u001B[39mcontext)\n\u001B[0;32m    264\u001B[0m x \u001B[38;5;241m=\u001B[39m rearrange(x, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mb (h w) c -> b c h w\u001B[39m\u001B[38;5;124m'\u001B[39m, h\u001B[38;5;241m=\u001B[39mh, w\u001B[38;5;241m=\u001B[39mw)\u001B[38;5;241m.\u001B[39mcontiguous()\n\u001B[0;32m    265\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mproj_out(x)\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32m~\\SoroushProjects\\01_Soroush_and_Shakiba\\NSD_High_Dimensional_Data\\11_Marco_And_Soroush\\versatile_diffusion\\lib\\model_zoo\\attention.py:212\u001B[0m, in \u001B[0;36mBasicTransformerBlock.forward\u001B[1;34m(self, x, context)\u001B[0m\n\u001B[0;32m    211\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x, context\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m--> 212\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m checkpoint(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward, (x, context), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mparameters(), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcheckpoint)\n",
      "File \u001B[1;32m~\\SoroushProjects\\01_Soroush_and_Shakiba\\NSD_High_Dimensional_Data\\11_Marco_And_Soroush\\versatile_diffusion\\lib\\model_zoo\\diffusion_utils.py:96\u001B[0m, in \u001B[0;36mcheckpoint\u001B[1;34m(func, inputs, params, flag)\u001B[0m\n\u001B[0;32m     94\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m flag:\n\u001B[0;32m     95\u001B[0m     args \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mtuple\u001B[39m(inputs) \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mtuple\u001B[39m(params)\n\u001B[1;32m---> 96\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m CheckpointFunction\u001B[38;5;241m.\u001B[39mapply(func, \u001B[38;5;28mlen\u001B[39m(inputs), \u001B[38;5;241m*\u001B[39margs)\n\u001B[0;32m     97\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     98\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39minputs)\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\autograd\\function.py:575\u001B[0m, in \u001B[0;36mFunction.apply\u001B[1;34m(cls, *args, **kwargs)\u001B[0m\n\u001B[0;32m    572\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_C\u001B[38;5;241m.\u001B[39m_are_functorch_transforms_active():\n\u001B[0;32m    573\u001B[0m     \u001B[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001B[39;00m\n\u001B[0;32m    574\u001B[0m     args \u001B[38;5;241m=\u001B[39m _functorch\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39munwrap_dead_wrappers(args)\n\u001B[1;32m--> 575\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39mapply(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m    577\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_setup_ctx_defined:\n\u001B[0;32m    578\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[0;32m    579\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIn order to use an autograd.Function with functorch transforms \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    580\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    581\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstaticmethod. For more details, please see \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    582\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttps://pytorch.org/docs/main/notes/extending.func.html\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    583\u001B[0m     )\n",
      "File \u001B[1;32m~\\SoroushProjects\\01_Soroush_and_Shakiba\\NSD_High_Dimensional_Data\\11_Marco_And_Soroush\\versatile_diffusion\\lib\\model_zoo\\diffusion_utils.py:108\u001B[0m, in \u001B[0;36mCheckpointFunction.forward\u001B[1;34m(ctx, run_function, length, *args)\u001B[0m\n\u001B[0;32m    105\u001B[0m ctx\u001B[38;5;241m.\u001B[39minput_params \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(args[length:])\n\u001B[0;32m    107\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[1;32m--> 108\u001B[0m     output_tensors \u001B[38;5;241m=\u001B[39m ctx\u001B[38;5;241m.\u001B[39mrun_function(\u001B[38;5;241m*\u001B[39mctx\u001B[38;5;241m.\u001B[39minput_tensors)\n\u001B[0;32m    109\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m output_tensors\n",
      "File \u001B[1;32m~\\SoroushProjects\\01_Soroush_and_Shakiba\\NSD_High_Dimensional_Data\\11_Marco_And_Soroush\\versatile_diffusion\\lib\\model_zoo\\attention.py:217\u001B[0m, in \u001B[0;36mBasicTransformerBlock._forward\u001B[1;34m(self, x, context)\u001B[0m\n\u001B[0;32m    215\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mattn1(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnorm1(x), context\u001B[38;5;241m=\u001B[39mcontext \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdisable_self_attn \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;241m+\u001B[39m x\n\u001B[0;32m    216\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mattn2(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnorm2(x), context\u001B[38;5;241m=\u001B[39mcontext) \u001B[38;5;241m+\u001B[39m x\n\u001B[1;32m--> 217\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mff(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnorm3(x)) \u001B[38;5;241m+\u001B[39m x\n\u001B[0;32m    218\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m x\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32m~\\SoroushProjects\\01_Soroush_and_Shakiba\\NSD_High_Dimensional_Data\\11_Marco_And_Soroush\\versatile_diffusion\\lib\\model_zoo\\attention.py:64\u001B[0m, in \u001B[0;36mFeedForward.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     63\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[1;32m---> 64\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnet(x)\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\container.py:250\u001B[0m, in \u001B[0;36mSequential.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    248\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[0;32m    249\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[1;32m--> 250\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m module(\u001B[38;5;28minput\u001B[39m)\n\u001B[0;32m    251\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\linear.py:125\u001B[0m, in \u001B[0;36mLinear.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    124\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 125\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mlinear(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mweight, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbias)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "88962172bbfba740"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
