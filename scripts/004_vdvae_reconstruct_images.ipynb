{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-15T10:05:29.674355Z",
     "start_time": "2025-04-15T10:05:25.055274Z"
    }
   },
   "source": [
    "path_model = 'C:/Users/sOrOush/SoroushProjects/02_Linearity_Project/02_Models/vdvae'\n",
    "\n",
    "import sys\n",
    "sys.path.append(path_model)\n",
    "# Check if the directory has been added\n",
    "print(\"Current sys.path:\", sys.path)\n",
    "\n",
    "# Attempt to import the module\n",
    "try:\n",
    "    from model_utils import *\n",
    "    print(\"Module model_utils successfully imported.\")\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing the module: {e}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current sys.path: ['C:\\\\Users\\\\sOrOush\\\\SoroushProjects\\\\14_CLIP_Ozcelic', 'C:\\\\Program Files\\\\JetBrains\\\\PyCharm 2024.3.2\\\\plugins\\\\python-ce\\\\helpers\\\\pydev', 'C:\\\\Program Files\\\\JetBrains\\\\PyCharm 2024.3.2\\\\plugins\\\\python\\\\helpers-pro\\\\jupyter_debug', 'C:\\\\Users\\\\sOrOush\\\\.conda\\\\envs\\\\linearity_question\\\\python312.zip', 'C:\\\\Users\\\\sOrOush\\\\.conda\\\\envs\\\\linearity_question\\\\DLLs', 'C:\\\\Users\\\\sOrOush\\\\.conda\\\\envs\\\\linearity_question\\\\Lib', 'C:\\\\Users\\\\sOrOush\\\\.conda\\\\envs\\\\linearity_question', '', 'C:\\\\Users\\\\sOrOush\\\\AppData\\\\Roaming\\\\Python\\\\Python312\\\\site-packages', 'C:\\\\Users\\\\sOrOush\\\\.conda\\\\envs\\\\linearity_question\\\\Lib\\\\site-packages', 'C:\\\\Users\\\\sOrOush\\\\.conda\\\\envs\\\\linearity_question\\\\Lib\\\\site-packages\\\\win32', 'C:\\\\Users\\\\sOrOush\\\\.conda\\\\envs\\\\linearity_question\\\\Lib\\\\site-packages\\\\win32\\\\lib', 'C:\\\\Users\\\\sOrOush\\\\.conda\\\\envs\\\\linearity_question\\\\Lib\\\\site-packages\\\\Pythonwin', 'C:\\\\Users\\\\sOrOush\\\\.conda\\\\envs\\\\linearity_question\\\\Lib\\\\site-packages\\\\setuptools\\\\_vendor', 'C:/Users/sOrOush/SoroushProjects/02_Linearity_Project/02_Models/vdvae']\n",
      "Module model_utils successfully imported.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T10:05:34.649090Z",
     "start_time": "2025-04-15T10:05:34.642859Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "#from mpi4py import MPI\n",
    "import socket\n",
    "import argparse\n",
    "import os\n",
    "import json\n",
    "import subprocess\n",
    "from hps import Hyperparams, parse_args_and_update_hparams, add_vae_arguments\n",
    "from utils import (logger,\n",
    "                   local_mpi_rank,\n",
    "                   mpi_size,\n",
    "                   maybe_download,\n",
    "                   mpi_rank)\n",
    "from data import mkdir_p\n",
    "from contextlib import contextmanager\n",
    "import torch.distributed as dist\n",
    "#from apex.optimizers import FusedAdam as AdamW\n",
    "from vae import VAE\n",
    "from torch.nn.parallel.distributed import DistributedDataParallel\n",
    "from train_helpers import restore_params\n",
    "from image_utils import *\n",
    "from model_utils import *\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "import pickle\n",
    "\n",
    "import argparse"
   ],
   "id": "f9c9839166aec763",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T10:05:47.296864Z",
     "start_time": "2025-04-15T10:05:47.291867Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check if the script is running in a Jupyter Notebook\n",
    "if 'ipykernel' in sys.modules:\n",
    "    # Default values for Jupyter Notebook\n",
    "    sub = 1  # You can specify the desired default value here\n",
    "    batch_size = 30  # Default value for batch size\n",
    "else:\n",
    "    # Argument parser for command-line execution\n",
    "    parser = argparse.ArgumentParser(description='Argument Parser')\n",
    "    parser.add_argument(\"-sub\", \"--sub\", help=\"Subject Number\", default=7)\n",
    "    parser.add_argument(\"-bs\", \"--bs\", help=\"Batch Size\", default=30)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Set the subject number and ensure it's valid\n",
    "    sub = int(args.sub)\n",
    "    assert sub in [1, 2, 5, 7]  # Validate allowed subject numbers\n",
    "\n",
    "    # Set the batch size\n",
    "    batch_size = int(args.bs)\n",
    "\n",
    "# Print status messages\n",
    "print('Libraries imported')\n",
    "print(f'Using Subject Number: {sub}')\n",
    "print(f'Using Batch Size: {batch_size}')\n"
   ],
   "id": "f8e79ff6f2581cc6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported\n",
      "Using Subject Number: 1\n",
      "Using Batch Size: 30\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T10:06:01.267391Z",
     "start_time": "2025-04-15T10:06:00.680448Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ref_latent = np.load(r\"C:\\Users\\sOrOush\\SoroushProjects\\14_CLIP_Ozcelic\\results\\extracted_fetures_31_Layered\\subj01\\ref_latents_31l.npz\".format(sub), allow_pickle=True)\n",
    "ref_latent = ref_latent[\"ref_latent\"]"
   ],
   "id": "20d9236eb9a98da8",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T10:06:08.072296Z",
     "start_time": "2025-04-15T10:06:06.801612Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as T\n",
    "\n",
    "# Define the configuration dictionary\n",
    "H = {\n",
    "    'image_size': 64, 'image_channels': 3, 'seed': 0, 'port': 29500, 'save_dir': './saved_models/test',\n",
    "    'data_root': './', 'desc': 'test', 'hparam_sets': 'imagenet64',\n",
    "    'restore_path': r'C:\\Users\\sOrOush\\SoroushProjects\\02_Linearity_Project\\02_Models\\vdvae\\model\\imagenet64-iter-1600000-model.th',\n",
    "    'restore_ema_path': r'C:\\Users\\sOrOush\\SoroushProjects\\02_Linearity_Project\\02_Models\\vdvae\\model\\imagenet64-iter-1600000-model-ema.th',\n",
    "    'restore_log_path': r'C:\\Users\\sOrOush\\SoroushProjects\\02_Linearity_Project\\02_Models\\vdvae\\model\\imagenet64-iter-1600000-log.jsonl',\n",
    "    'restore_optimizer_path': r'C:\\Users\\sOrOush\\SoroushProjects\\02_Linearity_Project\\02_Models\\vdvae\\model\\imagenet64-iter-1600000-opt.th',\n",
    "    'dataset': 'imagenet64', 'ema_rate': 0.999, 'enc_blocks': '64x11,64d2,32x20,32d2,16x9,16d2,8x8,8d2,4x7,4d4,1x5',\n",
    "    'dec_blocks': '1x2,4m1,4x3,8m4,8x7,16m8,16x15,32m16,32x31,64m32,64x12', 'zdim': 16, 'width': 512,\n",
    "    'custom_width_str': '', 'bottleneck_multiple': 0.25, 'no_bias_above': 64, 'scale_encblock': False,\n",
    "    'test_eval': True, 'warmup_iters': 100, 'num_mixtures': 10, 'grad_clip': 220.0, 'skip_threshold': 380.0,\n",
    "    'lr': 0.00015, 'lr_prior': 0.00015, 'wd': 0.01, 'wd_prior': 0.0, 'num_epochs': 10000, 'n_batch': 4,\n",
    "    'adam_beta1': 0.9, 'adam_beta2': 0.9, 'temperature': 1.0, 'iters_per_ckpt': 25000, 'iters_per_print': 1000,\n",
    "    'iters_per_save': 10000, 'iters_per_images': 10000, 'epochs_per_eval': 1, 'epochs_per_probe': None,\n",
    "    'epochs_per_eval_save': 1, 'num_images_visualize': 8, 'num_variables_visualize': 6, 'num_temperatures_visualize': 3,\n",
    "    'mpi_size': 1, 'local_rank': 0, 'rank': 0, 'logdir': './saved_models/test/log'\n",
    "}\n",
    "\n",
    "# Dot notation for dictionary\n",
    "class dotdict(dict):\n",
    "    \"\"\"dot.notation access to dictionary attributes\"\"\"\n",
    "    __getattr__ = dict.get\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__\n",
    "\n",
    "H = dotdict(H)\n",
    "\n",
    "# Assuming `set_up_data` and `load_vaes` functions are defined elsewhere\n",
    "H, preprocess_fn = set_up_data(H)\n",
    "\n",
    "print('Model is Loading')\n",
    "ema_vae = load_vaes(H)\n",
    "\n",
    "# Dataset class for external images\n",
    "class batch_generator_external_images(Dataset):\n",
    "    def __init__(self, data_path):\n",
    "        self.data_path = data_path\n",
    "        self.im = np.load(data_path).astype(np.uint8)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.fromarray(self.im[idx])\n",
    "        img = T.functional.resize(img, (64, 64))\n",
    "        img = torch.tensor(np.array(img)).float()\n",
    "        return img\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.im)\n"
   ],
   "id": "f72ae1043f6ad765",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is Loading\n",
      "Restoring ema vae from C:\\Users\\sOrOush\\SoroushProjects\\02_Linearity_Project\\02_Models\\vdvae\\model\\imagenet64-iter-1600000-model-ema.th\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users/sOrOush/SoroushProjects/02_Linearity_Project/02_Models/vdvae\\train_helpers.py:126: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(distributed_maybe_download(path, local_rank, mpi_size), map_location='cpu' if map_cpu else None)\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T12:19:50.861842Z",
     "start_time": "2025-04-15T12:19:50.821018Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Transfor latents from flattened representation to hierarchical\n",
    "def latent_transformation(latents, ref):\n",
    "  layer_dims = np.array([2**4,2**4,2**8,2**8,2**8,2**8,2**10,2**10,2**10,2**10,2**10,2**10,2**10,2**10,2**12,2**12,2**12,2**12,2**12,2**12,2**12,2**12,2**12,2**12,2**12,2**12,2**12,2**12,2**12,2**12,2**14])\n",
    "  transformed_latents = []\n",
    "  for i in range(31):\n",
    "    t_lat = latents[:,layer_dims[:i].sum():layer_dims[:i+1].sum()]\n",
    "    #std_norm_test_latent = (t_lat - np.mean(t_lat,axis=0)) / np.std(t_lat,axis=0)\n",
    "    #renorm_test_latent = std_norm_test_latent * np.std(kamitani_latents[i][num_test:].reshape(num_train,-1),axis=0) + np.mean(kamitani_latents[i][num_test:].reshape(num_train,-1),axis=0)\n",
    "    c,h,w=ref[i]['z'].shape[1:]\n",
    "    transformed_latents.append(t_lat.reshape(len(latents),c,h,w))\n",
    "  return transformed_latents\n",
    "\n",
    "# idx = range(len(test_images))\n",
    "# input_latent = latent_transformation(pred_latents[idx],ref_latent)\n",
    "\n",
    "\n",
    "def sample_from_hier_latents(latents,sample_ids):\n",
    "  sample_ids = [id for id in sample_ids if id<len(latents[0])]\n",
    "  layers_num=len(latents)\n",
    "  sample_latents = []\n",
    "  for i in range(layers_num):\n",
    "    sample_latents.append(torch.tensor(latents[i][sample_ids]).float().cuda())\n",
    "  return sample_latents\n"
   ],
   "id": "225fd357a54b08d3",
   "outputs": [],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T12:35:11.370055Z",
     "start_time": "2025-04-15T12:35:11.347572Z"
    }
   },
   "cell_type": "code",
   "source": "pred_latents = np.load(r\"C:\\Users\\sOrOush\\SoroushProjects\\14_CLIP_Ozcelic\\results\\polynomial_regression_results\\degree4\\layer_31_regression.npz\")",
   "id": "1ee6199bdd0ea0f9",
   "outputs": [],
   "execution_count": 95
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T12:35:17.205662Z",
     "start_time": "2025-04-15T12:35:17.034075Z"
    }
   },
   "cell_type": "code",
   "source": "pred_latents['predicted_test_latents'].shape",
   "id": "142a9e4a57138b3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(982, 91168)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 96
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T12:35:17.385906Z",
     "start_time": "2025-04-15T12:35:17.218308Z"
    }
   },
   "cell_type": "code",
   "source": "pred_latents = pred_latents['predicted_test_latents']",
   "id": "983663587b1377cf",
   "outputs": [],
   "execution_count": 97
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T12:35:23.012952Z",
     "start_time": "2025-04-15T12:35:22.933471Z"
    }
   },
   "cell_type": "code",
   "source": [
    "idx = range(len(pred_latents))\n",
    "input_latent = latent_transformation(pred_latents[idx],ref_latent)\n"
   ],
   "id": "3ba1ffe86df4bf88",
   "outputs": [],
   "execution_count": 98
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T12:35:28.447728Z",
     "start_time": "2025-04-15T12:35:28.442132Z"
    }
   },
   "cell_type": "code",
   "source": "idx",
   "id": "5b44904618f0b09c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(0, 982)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 99
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T12:35:33.887399Z",
     "start_time": "2025-04-15T12:35:33.884584Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Verzeichnis zum Speichern der generierten Bilder\n",
    "save_directory = 'C:/Users/sOrOush/SoroushProjects/14_CLIP_Ozcelic/results/generated images/VDVAE/Default/degree4/subj{:02d}'.format(sub) # no theta\n",
    "\n"
   ],
   "id": "9ab82fe5b56ddbe5",
   "outputs": [],
   "execution_count": 100
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T12:35:39.280997Z",
     "start_time": "2025-04-15T12:35:39.277128Z"
    }
   },
   "cell_type": "code",
   "source": "from tqdm.notebook import tqdm",
   "id": "ccc18295890c9248",
   "outputs": [],
   "execution_count": 101
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T12:35:44.600728Z",
     "start_time": "2025-04-15T12:35:44.597243Z"
    }
   },
   "cell_type": "code",
   "source": "#pip install ipywidgets --upgrade",
   "id": "e633968fd22ab1b3",
   "outputs": [],
   "execution_count": 102
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T12:37:29.871584Z",
     "start_time": "2025-04-15T12:35:50.337788Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Erstellen des Verzeichnisses, falls es nicht existiert\n",
    "os.makedirs(save_directory, exist_ok=True)\n",
    "\n",
    "for i in tqdm(range(int(np.ceil(len(pred_latents) / batch_size)))):\n",
    "    print(i * batch_size)\n",
    "    samp = sample_from_hier_latents(input_latent, range(i * batch_size, (i + 1) * batch_size))\n",
    "    px_z = ema_vae.decoder.forward_manual_latents(len(samp[0]), samp, t=None)\n",
    "    sample_from_latent = ema_vae.decoder.out_net.sample(px_z)\n",
    "\n",
    "    for j in range(len(sample_from_latent)):\n",
    "        im = sample_from_latent[j]\n",
    "        im = Image.fromarray(im)\n",
    "        im = im.resize((512, 512), resample=3)\n",
    "        im.save('{}/{}.png'.format(save_directory, i * batch_size + j))"
   ],
   "id": "2959ee5921219445",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/33 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "69694d993ea746868b4359938229ec39"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "120\n",
      "150\n",
      "180\n",
      "210\n",
      "240\n",
      "270\n",
      "300\n",
      "330\n",
      "360\n",
      "390\n",
      "420\n",
      "450\n",
      "480\n",
      "510\n",
      "540\n",
      "570\n",
      "600\n",
      "630\n",
      "660\n",
      "690\n",
      "720\n",
      "750\n",
      "780\n",
      "810\n",
      "840\n",
      "870\n",
      "900\n",
      "930\n",
      "960\n"
     ]
    }
   ],
   "execution_count": 103
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a49363926e9fb6e6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
